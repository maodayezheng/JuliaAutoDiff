#function DemoMNISTclass()
    # Training a deep classifier on MNIST
    # The method uses Nesterov's accelerated gradient, with minibatches
    # David Barber, University College London 2015

    # If running from repl within julia, need to first run:
    # using dbAutoDiff, Winston
    # include("DemoMNISTclass.jl);

    Ntrain=2000
    BatchSize=500
    TrainingIts=500 # number of Nesterov updates

    #Ntrain=2
    #BatchSize=2
    #TrainingIts=2 # number of Nesterov updates
    include("loadmnist.jl")
    images,label=loadmnist()
    r=randperm(size(images,2))
    data=images[:,r]
    class=-ones(1,70000)
    for i=1:70000
        if label[i]==2 # classify as a 2 or non-2 digit
            class[i]=1
        end
    end
    class=class[:,r]

    # Construct the DAG function:
    H=[784 250 100 50 1] # number of units in each layer
    #H=[784 5 1] # number of units in each layer
    L=length(H) # number of hidden layers
    # node indices:
    w=zeros(Int,L) # weight index
    h=zeros(Int,L) # hidden layer index (note that I call the input layer h[1])

    # nodes which are inputs have empty parent set
    # note that the ordering is irrelevant as long as this is a DAG
    # and that the scalar loss is the last node in the DAG
    nMAX=100 # maximum number of nodes in the DAG
    c=0 # node counter
    node=Array(ADnode,nMAX) # nodes
    node[xtrain=h[1]=c+=1]=ADnode([]) # node has no parents
    node[classtrain=c+=1]=ADnode([]) # node has no parents

    for i=2:L-1
        node[w[i]=c+=1]=ADnode([];returnderivative=true) # node is a parameter
        node[h[i]=c+=1]=ADnode([w[i] h[i-1]],FshiftedsigmaAx) # don't need derivatives of hidden layer
    end
    node[w[L]=c+=1]=ADnode([];returnderivative=true) # node is a parameter
    node[h[L]=c+=1]=ADnode([w[L] h[L-1]],FAx)

    macro regularise(lambda,regnodes)
        println(c)
        q=quote
            node[totalreg=c+=1]=ADnode($(regnodes)[1],FmeanAbs)
            for i=1:length($(regnodes))
                println(i)
                node[thisreg=c+=1]=ADnode($(regnodes)[i],FmeanAbs)
                node[totalreg=c+=1]=ADnode([totalreg thisreg],Fxpy)
            end
            node[reglambda=c+=1]=ADnode([])
            node[regpen=c+=1]=ADnode([totalreg reglambda],Fxy) # This is the final regularisation penalty
        end
    end

    function mapreduce!(f,op,nodeinds,node::Array{ADnode,1})
        counter=endnode(node)
        node[red=counter+=1]=ADnode(nodeinds[1],f)
        for i=2:length(nodeinds)
            node[thisred=counter+=1]=ADnode(nodeinds[i],f)
            node[red=counter+=1]=ADnode([red thisred],op)
        end
        return red
    end


    end
    # regulariser (pretty clunky to do it this way. This is a consequence of using messages functions from children to parents that can change depending on which parent we send the message to):
    regnode=w[2:L]# which nodes are included in the regularisation term
    node[totalreg=c+=1]=ADnode(regnode[1],FmeanAbs)
    for i=2:length(regnode)
        node[thisreg=c+=1]=ADnode(regnode[i],FmeanAbs)
        node[totalreg=c+=1]=ADnode([totalreg thisreg],Fxpy)
    end
    node[reglambda=c+=1]=ADnode([])
    node[regpen=c+=1]=ADnode([totalreg reglambda],Fxy) # This is the final regularisation penalty

    node[logloss=c+=1]=ADnode([classtrain h[L]],FLogisticLoss)
    node[loss=c+=1]=ADnode([logloss regpen],Fxpy) # the loss we are minimising must be the final node in the graph.


    node=node[1:c] # just take only the nodes required

    # instantiate parameter nodes and inputs:
    value=Array(Any,c) # function values on the nodes
    value[xtrain]=data[:,1:BatchSize];
    value[classtrain]=class[:,1:BatchSize];
    for i=2:L
        value[w[i]]=sign(randn(H[i],H[i-1]))/sqrt(H[i-1])
    end
    value[reglambda]=0.1 # regularisation lambda

    (value,auxvalue,gradient,net)=compile(value,node); # compile the DAG and preallocate memory

    #gradcheck(value,net) # use a small number of datapoints and small network to check the gradient, otherwise this will be very slow

    # Nesterov Training:
    parstoupdate=find(map(x->x.returnderivative,net.node)) # node indices that are parameters
    er=zeros(TrainingIts)
    println("Batch Nesterov with decaying learning rate")
    tic()
    minibatchstart=1 # starting datapoint for the minibatch
    ADeval!(value,net; auxvalue=auxvalue,gradient=gradient) # ensure all values are computed. The named arguments are not necessary, but prevent garbage collection
    nesterov=0*gradient
    valueold=deepcopy(value)
    oldnesterov=deepcopy(nesterov)
    for t=1:TrainingIts
        if minibatchstart>Ntrain-BatchSize+1
            minibatchstart=Ntrain-BatchSize+1
        end
        minibatch=minibatchstart:minibatchstart+BatchSize-1
        minibatchstart=minibatchstart+BatchSize
        if minibatchstart>Ntrain
            minibatchstart=1
        end
        value[xtrain]=data[:,minibatch] # select batch
        value[classtrain]=class[:,minibatch]

        # Nesterov Accelerated Gradient update:
        mu=1-3/(t+5);
        if t>10
            epsilon=0.01/(1+t/100)
        else
            epsilon=0.01
        end

        copy!(valueold,value)
        copyind!(oldnesterov,nesterov,parstoupdate)
        copyind!(value,value+mu*oldnesterov,parstoupdate)
        ADeval!(value,net;auxvalue=auxvalue,gradient=gradient)
        copyind!(nesterov,mu*oldnesterov-epsilon*gradient,parstoupdate)
        copyind!(value,valueold+nesterov,parstoupdate)

        er[t]=value[loss]
        println("[$(t)]Loss = $(value[loss]),  learning rate=$epsilon")
    end
    toc()
    classout=ones(1,length(value[classtrain]))
    classout[find(sigma(value[h[L]]).<0.5)]=-1
    classtrue=value[classtrain]
    println("training accuracy = $(mean(classout==classtrue))")

#end
